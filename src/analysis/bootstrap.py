from prodict import Prodict
from sklearn.metrics import mean_squared_error, r2_score

import numpy as np
import pandas as pd
import random
import sys
import utils

sys.path.append("src")


def perform_bootstrap(cfg: Prodict) -> None:
    """Generates bootstrapped performance metrics between a series of predictions and observations.

    Parameters
    ----------
    cfg : Prodict
        A configuration file specifying the parameters of the evaluation.

    Raises
    ------
    Exception
        FileNotFoundError with reference to the csv file generated by `evaluate_model`.
    """
    print("Generating bootstrapped performance metrics.")
    bootstrap_dir = (
        f"{cfg.DATA.OUTPUT_DIR}/{cfg.TEST.RUN_ID}/e{cfg.TEST.EPOCH_TO_TEST}/tables/bootstrap"
    )
    utils.make_dir(bootstrap_dir)
    table_path = f"{cfg.DATA.OUTPUT_DIR}/{cfg.TEST.RUN_ID}/e{cfg.TEST.EPOCH_TO_TEST}/tables/epoch-{cfg.TEST.EPOCH_TO_TEST}_cutoff-{cfg.DATA.CUTOFF}_cohort-{cfg.DATA.COHORT}_arm-{cfg.DATA.ARM}_evalall-{cfg.TEST.EVAL_ALL}_results.csv"
    summary_path = f"{bootstrap_dir}/epoch-{cfg.TEST.EPOCH_TO_TEST}_cutoff-{cfg.DATA.CUTOFF}_cohort-{cfg.DATA.COHORT}_n-{cfg.TEST.BOOTSTRAP.NUM_SAMPLE}_arm-{cfg.DATA.ARM}_evalall-{cfg.TEST.EVAL_ALL}_summary_statistics.csv"
    results_path = f"{bootstrap_dir}/epoch-{cfg.TEST.EPOCH_TO_TEST}_cutoff-{cfg.DATA.CUTOFF}_cohort-{cfg.DATA.COHORT}_bootstrap_n-{cfg.TEST.BOOTSTRAP.NUM_SAMPLE}_arm-{cfg.DATA.ARM}_evalall-{cfg.TEST.EVAL_ALL}.csv"
    try:
        df = pd.read_csv(table_path)
    except FileNotFoundError:
        print(table_path)
        raise Exception(
            "No predictions file found. Please check your directories in the configuration file."
        )

    labels = np.array(df.labels.values)
    preds = np.array(df.preds.values)
    (rmse, r2) = np.array([]), np.array([])
    for _ in range(cfg.TEST.BOOTSTRAP.NUM_SAMPLE):
        sample_idx = random.choices(range(preds.shape[0]), k=preds.shape[0])
        label_sample = labels[sample_idx]
        preds_sample = preds[sample_idx]
        rmse_sample = mean_squared_error(label_sample, preds_sample, squared=False)
        r2_sample = r2_score(label_sample, preds_sample)
        rmse = np.append(rmse, rmse_sample)
        r2 = np.append(r2, r2_sample)
    (rmse_mean, r2_mean) = np.mean(rmse), np.mean(r2)
    (rmse_std, r2_std) = np.std(rmse), np.std(r2)
    (rmse_median, r2_median) = np.median(rmse), np.median(r2)

    pd.DataFrame({"RMSE": rmse, "R2": r2}).to_csv(results_path, sep="\t")
    pd.DataFrame(
        {
            "RMSE_mean": [rmse_mean],
            "RMSE_std": [rmse_std],
            "RMSE_median": [rmse_median],
            "R2_mean": [r2_mean],
            "R2_std": [r2_std],
            "R2_median": [r2_median],
        }
    ).round(decimals=4).to_csv(summary_path, sep="\t")
